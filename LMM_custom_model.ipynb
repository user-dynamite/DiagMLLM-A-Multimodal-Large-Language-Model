{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/gil/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/gil/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Error loading pun`kt_tab: Package 'pun`kt_tab' not found\n",
      "[nltk_data]     in index\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchvision.transforms as transforms\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from glob import glob\n",
    "import pickle\n",
    "import nltk\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from efficientnet_pytorch_3d import EfficientNet3D\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('pun`kt_tab')\n",
    "\n",
    "device = torch.device('cuda:5' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'image_size':512,\n",
    "        'lr':1e-4,\n",
    "        'beta1':0.5,\n",
    "        'beta2':0.999,\n",
    "        'batch_size':5,\n",
    "        'epochs':300,\n",
    "        'image_count':50,\n",
    "        'data_path':'../data/NIA7/',\n",
    "        'train_csv':'fold_5_train.csv',\n",
    "        'val_csv':'fold_5_val.csv',\n",
    "        'vocab_path':'../data/vocab.pkl',\n",
    "        'embed_size':300,\n",
    "        'hidden_size':256,\n",
    "        'num_layers':1,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path, image_count, image_size, csv, class_dataset, vocab, transform=None):\n",
    "        self.root = data_path\n",
    "        self.image_count = image_count\n",
    "        self.df = pd.read_csv(data_path + csv)\n",
    "        self.class_dataset = class_dataset\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # 'age', 'height', 'weight' 컬럼을 MinMaxScaler로 스케일링\n",
    "        self.df['sex'] = self.df['sex'].map({'M': 0, 'F': 1})\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.df[['age', 'sex', 'height', 'weight']] = self.scaler.fit_transform(self.df[['age', 'sex', 'height', 'weight']])\n",
    "        \n",
    "        # 모든 데이터를 메모리에 캐싱\n",
    "        self.cached_data = []\n",
    "        for index, row in tqdm(self.df.iterrows(), total=len(self.df), desc=\"Caching data\"):\n",
    "            patient_id = row['ID']\n",
    "            \n",
    "            # CT 이미지(3D 데이터) 로드 및 캐싱\n",
    "            ct_image_paths = glob(self.root + f'{patient_id}/02. CT/*.png')\n",
    "            ct_images = torch.zeros(self.image_count, 1, self.image_size, self.image_size)\n",
    "            ct_image_index = torch.randint(low=0, high=len(ct_image_paths), size=(self.image_count,))\n",
    "            for count, idx in enumerate(ct_image_index):\n",
    "                ct_image = Image.open(ct_image_paths[idx]).convert('L')\n",
    "                if self.transform is not None:\n",
    "                    ct_image = self.transform(ct_image)\n",
    "                ct_images[count] = ct_image\n",
    "\n",
    "            ct_images = ct_images.permute(1, 0, 2, 3)  # 형태: (1, image_count, image_size, image_size)\n",
    "\n",
    "            # CR 이미지(2D 데이터) 로드 및 캐싱\n",
    "            cr_image_path = glob(self.root + f'{patient_id}/01. CR/*.png')[0]\n",
    "            cr_image = Image.open(cr_image_path).convert('L')\n",
    "            if self.transform is not None:\n",
    "                cr_image = self.transform(cr_image)\n",
    "\n",
    "            # 캡션을 토큰 ID로 변환하여 캐싱\n",
    "            caption = row['text']\n",
    "            tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "            caption_tokens = [vocab('<start>')] + [vocab(token) for token in tokens] + [vocab('<end>')]\n",
    "            target = torch.Tensor(caption_tokens)\n",
    "\n",
    "            # 'age', 'height', 'weight' 값 캐싱\n",
    "            sex = torch.tensor(row['sex'], dtype=torch.float32)\n",
    "            age = torch.tensor(row['age'], dtype=torch.float32)\n",
    "            height = torch.tensor(row['height'], dtype=torch.float32)\n",
    "            weight = torch.tensor(row['weight'], dtype=torch.float32)\n",
    "\n",
    "            # 캐싱된 데이터 저장\n",
    "            self.cached_data.append((ct_images, cr_image, target, sex, age, height, weight))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 캐싱된 데이터에서 인덱스로 직접 접근\n",
    "        return self.cached_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def collate_fn(data):\n",
    "    # Sort data by caption length (descending order).\n",
    "    data.sort(key=lambda x: len(x[2]), reverse=True)\n",
    "    ct_images, cr_images, captions, sexs, ages, heights, weights = zip(*data)\n",
    "\n",
    "    # Merge images\n",
    "    ct_images = torch.stack(ct_images, 0)\n",
    "    cr_images = torch.stack(cr_images, 0)\n",
    "\n",
    "    # Merge captions\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "\n",
    "    # Stack ages and heights\n",
    "    sexs = torch.stack(sexs, 0)\n",
    "    ages = torch.stack(ages, 0)\n",
    "    heights = torch.stack(heights, 0)\n",
    "    weights = torch.stack(weights, 0)\n",
    "\n",
    "    return ct_images, cr_images, targets, lengths, sexs, ages, heights, weights\n",
    "\n",
    "def idx2word(vocab, indices):\n",
    "    sentence = []\n",
    "    for i in range(params['batch_size']):\n",
    "        indices[i].cpu().numpy()\n",
    "    \n",
    "    for index in indices:\n",
    "        word = vocab.idx2word[index]\n",
    "        sentence.append(word)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= Encoder - 2D Feature Extractor (CNN) =======\n",
    "class FeatureExtractor2D(nn.Module):\n",
    "    \"\"\"2D Feature extractor block for X-ray images (simple CNN version)\"\"\"\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(FeatureExtractor2D, self).__init__()\n",
    "        self.conv2d_1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2d_2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2d_3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)  # 2x downsampling\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer 추가\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv2d_1(inputs)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "\n",
    "        x = self.conv2d_2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "\n",
    "        x = self.conv2d_3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        features = self.flatten(x)\n",
    "        features = self.dropout(features)  # Apply dropout at the end\n",
    "        return features\n",
    "    \n",
    "\n",
    "# ======= Encoder - 3D Feature Extractor =======\n",
    "class FeatureExtractor3D(nn.Module):\n",
    "    \"\"\"3D Feature extractor block for CT images\"\"\"\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(FeatureExtractor3D, self).__init__()\n",
    "        self.conv3d_1 = nn.Conv3d(in_channels=1, out_channels=16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
    "        self.conv3d_2 = nn.Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
    "        self.conv3d_3 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
    "        self.pool = nn.MaxPool3d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer 추가\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv3d_1(inputs)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "\n",
    "        x = self.conv3d_2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "\n",
    "        x = self.conv3d_3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        features = self.flatten(x)  # Flatten the output to a 2D tensor\n",
    "        features = self.dropout(features)  # Apply dropout at the end\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMILModel(nn.Module):\n",
    "    def __init__(self, image_feature_dim_2d, image_feature_dim_3d, \n",
    "                 feature_extractor_2d: nn.Module, feature_extractor_3d: nn.Module,\n",
    "                 age_height_weight_input_size=4, dropout_rate=0.3):\n",
    "        super(AttentionMILModel, self).__init__()\n",
    "        self.feature_extractor_2d = feature_extractor_2d\n",
    "        self.feature_extractor_3d = feature_extractor_3d\n",
    "\n",
    "        self.age_height_weight_mlp = nn.Sequential(\n",
    "            nn.Linear(age_height_weight_input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        self.combined_feature_dim = image_feature_dim_2d + image_feature_dim_3d + 32\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Attention + projection for T5\n",
    "        self.attention_2d = nn.Sequential(\n",
    "            nn.Linear(self.combined_feature_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.feature_proj = nn.Linear(self.combined_feature_dim, 512)  # ✅ T5 input projection\n",
    "\n",
    "    def forward(self, inputs_2d, inputs_3d, age_height_weight):\n",
    "        features_2d = self.feature_extractor_2d(inputs_2d)\n",
    "        features_3d = self.feature_extractor_3d(inputs_3d)\n",
    "        features_meta = self.age_height_weight_mlp(age_height_weight)\n",
    "\n",
    "        features = torch.cat([features_2d, features_3d, features_meta], dim=1)\n",
    "\n",
    "        attention_weights = self.attention_2d(features)\n",
    "        attention_weights = F.softmax(attention_weights, dim=1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        attended = features * attention_weights\n",
    "        attended = self.dropout(attended)\n",
    "\n",
    "        projected = self.feature_proj(attended)  # [B, 512]\n",
    "        return projected  # ❗ T5 decoder에 넘기기 전에는 .unsqueeze(1) 필요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Model, T5Config\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class SwiGLUFFN(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, dropout_rate=0.1):\n",
    "        super(SwiGLUFFN, self).__init__()\n",
    "        self.gate_proj = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.up_proj = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.down_proj = nn.Linear(intermediate_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = F.silu(self.gate_proj(x))\n",
    "        up = self.up_proj(x)\n",
    "        x = gate * up\n",
    "        x = self.down_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class ReducedDecoderT5SwiGLU(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_length=50, dropout_rate=0.7):\n",
    "        super(ReducedDecoderT5SwiGLU, self).__init__()\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        config = T5Config(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=512,\n",
    "            d_ff=2048,\n",
    "            num_layers=6,\n",
    "            num_heads=8,\n",
    "            dropout_rate=dropout_rate,\n",
    "            is_encoder_decoder=True,\n",
    "            output_attentions=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        self.transformer = T5Model(config)\n",
    "        self.lm_head = nn.Linear(config.d_model, vocab_size, bias=False)\n",
    "\n",
    "        # Replace FFN with SwiGLUFFN\n",
    "        for block in self.transformer.decoder.block:\n",
    "            hidden_size = config.d_model\n",
    "            intermediate_size = config.d_ff\n",
    "            block.layer[1].DenseReluDense = SwiGLUFFN(hidden_size, intermediate_size, dropout_rate)\n",
    "\n",
    "    def forward(self, encoder_hidden_states, decoder_input_ids, decoder_attention_mask=None):\n",
    "        decoder_outputs = self.transformer(\n",
    "            encoder_outputs=(encoder_hidden_states,),  # <-- 핵심\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask\n",
    "        )\n",
    "        sequence_output = decoder_outputs.last_hidden_state\n",
    "        logits = self.lm_head(sequence_output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(params['vocab_path'], 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "transform = transforms.Compose([ \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5,), (0.5,))  # Adjust for grayscale images with 1 channel\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature_Extractor2D = FeatureExtractor2D()\n",
    "Feature_Extractor3D = FeatureExtractor3D()\n",
    "\n",
    "# Initialize encoder and new decoder\n",
    "encoder = AttentionMILModel(65536, 196608, Feature_Extractor2D, Feature_Extractor3D).to(device)\n",
    "decoder = ReducedDecoderT5SwiGLU(len(vocab)).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model_param = list(decoder.parameters()) + list(encoder.parameters())\n",
    "optimizer = torch.optim.Adam(model_param, lr=params['lr'], betas=(params['beta1'], params['beta2']), weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureExtractor2D feature size: torch.Size([1, 65536])\n"
     ]
    }
   ],
   "source": [
    "# EfficientNet B3의 FeatureExtractor2D 출력 크기 확인\n",
    "sample_input = torch.randn(1, 1, params['image_size'], params['image_size']).to(device)\n",
    "features_2d = Feature_Extractor2D(sample_input)\n",
    "print(\"FeatureExtractor2D feature size:\", features_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureExtractor3D output size: torch.Size([1, 196608])\n"
     ]
    }
   ],
   "source": [
    "# FeatureExtractor3D의 출력 크기 확인\n",
    "sample_ct_input = torch.randn(1, 1, params['image_count'], params['image_size'], params['image_size']).to(device)\n",
    "features_3d = Feature_Extractor3D(sample_ct_input)\n",
    "print(\"FeatureExtractor3D output size:\", features_3d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# caching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching data: 100%|██████████| 649/649 [33:17<00:00,  3.08s/it] \n",
      "Caching data: 100%|██████████| 163/163 [05:04<00:00,  1.87s/it]\n"
     ]
    }
   ],
   "source": [
    "train_dataset=CustomDataset(params['data_path'],params['image_count'],params['image_size'],params['train_csv'],'train',vocab,transform=transform)\n",
    "test_dataset=CustomDataset(params['data_path'],params['image_count'],params['image_size'],params['val_csv'],'val',vocab,transform=transform)\n",
    "train_dataloader=DataLoader(train_dataset,batch_size=params['batch_size'],shuffle=True,collate_fn=collate_fn)\n",
    "val_dataloader=DataLoader(test_dataset,batch_size=params['batch_size'],shuffle=True,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/130 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1/300 Step: 131 loss : 0.3850 : 100%|██████████| 130/130 [01:40<00:00,  1.29it/s]\n",
      "epoch: 1/300 Step: 34 loss : 0.1400 : 100%|██████████| 33/33 [00:06<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Validation Loss: 0.1400\n",
      "Current learning rate: 0.0001\n",
      "Best model saved with Validation Loss: 4.620517782866955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 2/300 Step: 131 loss : 0.3740 : 100%|██████████| 130/130 [01:40<00:00,  1.29it/s]\n",
      "epoch: 2/300 Step: 34 loss : 0.1332 : 100%|██████████| 33/33 [00:06<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/300], Validation Loss: 0.1332\n",
      "Current learning rate: 0.0001\n",
      "Best model saved with Validation Loss: 4.394742090255022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 3/300 Step: 131 loss : 0.3620 : 100%|██████████| 130/130 [01:40<00:00,  1.29it/s]\n",
      "epoch: 3/300 Step: 34 loss : 0.1386 : 100%|██████████| 33/33 [00:06<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/300], Validation Loss: 0.1386\n",
      "Current learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 4/300 Step: 131 loss : 0.3520 : 100%|██████████| 130/130 [01:40<00:00,  1.30it/s]\n",
      "epoch: 4/300 Step: 34 loss : 0.1273 : 100%|██████████| 33/33 [00:06<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/300], Validation Loss: 0.1273\n",
      "Current learning rate: 0.0001\n",
      "Best model saved with Validation Loss: 4.20126311853528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 5/300 Step: 131 loss : 0.3392 : 100%|██████████| 130/130 [01:40<00:00,  1.29it/s]\n",
      "epoch: 5/300 Step: 34 loss : 0.1177 : 100%|██████████| 33/33 [00:06<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/300], Validation Loss: 0.1177\n",
      "Current learning rate: 0.0001\n",
      "Best model saved with Validation Loss: 3.8852624744176865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 6/300 Step: 131 loss : 0.3930 : 100%|██████████| 130/130 [01:40<00:00,  1.29it/s]\n",
      "epoch: 6/300 Step: 34 loss : 0.1112 : 100%|██████████| 33/33 [00:06<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/300], Validation Loss: 0.1112\n",
      "Current learning rate: 0.0001\n",
      "Best model saved with Validation Loss: 3.669293670915067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 7/300 Step: 131 loss : 0.3144 : 100%|██████████| 130/130 [01:40<00:00,  1.29it/s]\n",
      "epoch: 7/300 Step: 34 loss : 0.1037 : 100%|██████████| 33/33 [00:06<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/300], Validation Loss: 0.1037\n",
      "Current learning rate: 0.0001\n",
      "Best model saved with Validation Loss: 3.423365879803896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 8/300 Step: 131 loss : 0.3051 : 100%|██████████| 130/130 [01:40<00:00,  1.29it/s]\n",
      "epoch: 8/300 Step: 34 loss : 0.1095 : 100%|██████████| 33/33 [00:06<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/300], Validation Loss: 0.1095\n",
      "Current learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 9/300 Step: 131 loss : 0.2935 : 100%|██████████| 130/130 [01:40<00:00,  1.30it/s]\n",
      "epoch: 9/300 Step: 34 loss : 0.1040 : 100%|██████████| 33/33 [00:06<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/300], Validation Loss: 0.1040\n",
      "Current learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 10/300 Step: 131 loss : 0.2897 : 100%|██████████| 130/130 [01:40<00:00,  1.30it/s]\n",
      "epoch: 10/300 Step: 34 loss : 0.1038 : 100%|██████████| 33/33 [00:06<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Validation Loss: 0.1038\n",
      "Current learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 11/300 Step: 131 loss : 0.2852 : 100%|██████████| 130/130 [01:40<00:00,  1.29it/s]\n",
      "epoch: 11/300 Step: 34 loss : 0.1037 : 100%|██████████| 33/33 [00:06<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/300], Validation Loss: 0.1037\n",
      "Current learning rate: 5e-05\n",
      "Best model saved with Validation Loss: 3.4229789823293686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 12/300 Step: 131 loss : 0.2756 : 100%|██████████| 130/130 [01:40<00:00,  1.29it/s]\n",
      "epoch: 12/300 Step: 34 loss : 0.0989 : 100%|██████████| 33/33 [00:06<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/300], Validation Loss: 0.0989\n",
      "Current learning rate: 5e-05\n",
      "Best model saved with Validation Loss: 3.2633445411920547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 13/300 Step: 131 loss : 0.2745 : 100%|██████████| 130/130 [01:40<00:00,  1.29it/s]\n",
      "epoch: 13/300 Step: 34 loss : 0.1028 : 100%|██████████| 33/33 [00:06<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/300], Validation Loss: 0.1028\n",
      "Current learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 14/300 Step: 131 loss : 0.2659 : 100%|██████████| 130/130 [01:40<00:00,  1.29it/s]\n",
      "epoch: 14/300 Step: 34 loss : 0.0948 : 100%|██████████| 33/33 [00:06<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/300], Validation Loss: 0.0948\n",
      "Current learning rate: 5e-05\n",
      "Best model saved with Validation Loss: 3.1277591474354267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 15/300 Step: 131 loss : 0.2589 : 100%|██████████| 130/130 [01:40<00:00,  1.29it/s]\n",
      "epoch: 15/300 Step: 34 loss : 0.0932 : 100%|██████████| 33/33 [00:06<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/300], Validation Loss: 0.0932\n",
      "Current learning rate: 5e-05\n",
      "Best model saved with Validation Loss: 3.074034094810486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 16/300 Step: 131 loss : 0.2561 : 100%|██████████| 130/130 [01:40<00:00,  1.29it/s]\n",
      "epoch: 16/300 Step: 34 loss : 0.0876 : 100%|██████████| 33/33 [00:06<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/300], Validation Loss: 0.0876\n",
      "Current learning rate: 5e-05\n",
      "Best model saved with Validation Loss: 2.892386980354786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 17/300 Step: 131 loss : 0.2557 : 100%|██████████| 130/130 [01:40<00:00,  1.29it/s]\n",
      "epoch: 17/300 Step: 34 loss : 0.0878 : 100%|██████████| 33/33 [00:06<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/300], Validation Loss: 0.0878\n",
      "Current learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 18/300 Step: 131 loss : 0.2514 : 100%|██████████| 130/130 [01:40<00:00,  1.29it/s]\n",
      "epoch: 18/300 Step: 34 loss : 0.0905 : 100%|██████████| 33/33 [00:06<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/300], Validation Loss: 0.0905\n",
      "Current learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 19/300 Step: 131 loss : 0.2424 : 100%|██████████| 130/130 [01:40<00:00,  1.30it/s]\n",
      "epoch: 19/300 Step: 34 loss : 0.0903 : 100%|██████████| 33/33 [00:06<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/300], Validation Loss: 0.0903\n",
      "Current learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 20/300 Step: 131 loss : 0.2449 : 100%|██████████| 130/130 [01:40<00:00,  1.30it/s]\n",
      "epoch: 20/300 Step: 34 loss : 0.0897 : 100%|██████████| 33/33 [00:06<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/300], Validation Loss: 0.0897\n",
      "Current learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 21/300 Step: 131 loss : 0.2394 : 100%|██████████| 130/130 [01:40<00:00,  1.29it/s]\n",
      "epoch: 21/300 Step: 34 loss : 0.0834 : 100%|██████████| 33/33 [00:06<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/300], Validation Loss: 0.0834\n",
      "Current learning rate: 1.25e-05\n",
      "Best model saved with Validation Loss: 2.7535200230777264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 22/300 Step: 131 loss : 0.2361 : 100%|██████████| 130/130 [01:40<00:00,  1.30it/s]\n",
      "epoch: 22/300 Step: 34 loss : 0.0854 : 100%|██████████| 33/33 [00:06<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/300], Validation Loss: 0.0854\n",
      "Current learning rate: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 23/300 Step: 131 loss : 0.2376 : 100%|██████████| 130/130 [01:40<00:00,  1.29it/s]\n",
      "epoch: 23/300 Step: 34 loss : 0.0843 : 100%|██████████| 33/33 [00:06<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/300], Validation Loss: 0.0843\n",
      "Current learning rate: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 24/300 Step: 131 loss : 0.2358 : 100%|██████████| 130/130 [01:40<00:00,  1.30it/s]\n",
      "epoch: 24/300 Step: 34 loss : 0.0862 : 100%|██████████| 33/33 [00:06<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/300], Validation Loss: 0.0862\n",
      "Current learning rate: 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 25/300 Step: 131 loss : 0.2369 : 100%|██████████| 130/130 [01:39<00:00,  1.30it/s]\n",
      "epoch: 25/300 Step: 34 loss : 0.0841 : 100%|██████████| 33/33 [00:06<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/300], Validation Loss: 0.0841\n",
      "Current learning rate: 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 26/300 Step: 131 loss : 0.2339 : 100%|██████████| 130/130 [01:39<00:00,  1.30it/s]\n",
      "epoch: 26/300 Step: 34 loss : 0.0865 : 100%|██████████| 33/33 [00:06<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/300], Validation Loss: 0.0865\n",
      "Current learning rate: 3.125e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 27/300 Step: 131 loss : 0.2406 : 100%|██████████| 130/130 [01:40<00:00,  1.30it/s]\n",
      "epoch: 27/300 Step: 34 loss : 0.0842 : 100%|██████████| 33/33 [00:06<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/300], Validation Loss: 0.0842\n",
      "Current learning rate: 3.125e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 28/300 Step: 131 loss : 0.2366 : 100%|██████████| 130/130 [01:41<00:00,  1.28it/s]\n",
      "epoch: 28/300 Step: 34 loss : 0.0866 : 100%|██████████| 33/33 [00:06<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/300], Validation Loss: 0.0866\n",
      "Current learning rate: 1.5625e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 29/300 Step: 131 loss : 0.2362 : 100%|██████████| 130/130 [01:40<00:00,  1.30it/s]\n",
      "epoch: 29/300 Step: 34 loss : 0.0836 : 100%|██████████| 33/33 [00:06<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/300], Validation Loss: 0.0836\n",
      "Current learning rate: 1.5625e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 30/300 Step: 131 loss : 0.2345 : 100%|██████████| 130/130 [01:39<00:00,  1.30it/s]\n",
      "epoch: 30/300 Step: 34 loss : 0.0837 : 100%|██████████| 33/33 [00:06<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/300], Validation Loss: 0.0837\n",
      "Current learning rate: 7.8125e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 31/300 Step: 131 loss : 0.2330 : 100%|██████████| 130/130 [01:40<00:00,  1.29it/s]\n",
      "epoch: 31/300 Step: 34 loss : 0.0810 : 100%|██████████| 33/33 [00:06<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/300], Validation Loss: 0.0810\n",
      "Current learning rate: 7.8125e-07\n",
      "Best model saved with Validation Loss: 2.673199510201812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 32/300 Step: 131 loss : 0.2352 : 100%|██████████| 130/130 [01:39<00:00,  1.31it/s]\n",
      "epoch: 32/300 Step: 34 loss : 0.0860 : 100%|██████████| 33/33 [00:06<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/300], Validation Loss: 0.0860\n",
      "Current learning rate: 7.8125e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 33/300 Step: 131 loss : 0.2327 : 100%|██████████| 130/130 [01:39<00:00,  1.31it/s]\n",
      "epoch: 33/300 Step: 34 loss : 0.0872 : 100%|██████████| 33/33 [00:06<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/300], Validation Loss: 0.0872\n",
      "Current learning rate: 7.8125e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 34/300 Step: 131 loss : 0.2347 : 100%|██████████| 130/130 [01:39<00:00,  1.31it/s]\n",
      "epoch: 34/300 Step: 34 loss : 0.0814 : 100%|██████████| 33/33 [00:06<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/300], Validation Loss: 0.0814\n",
      "Current learning rate: 3.90625e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 35/300 Step: 131 loss : 0.2336 : 100%|██████████| 130/130 [01:39<00:00,  1.31it/s]\n",
      "epoch: 35/300 Step: 34 loss : 0.0821 : 100%|██████████| 33/33 [00:06<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/300], Validation Loss: 0.0821\n",
      "Current learning rate: 3.90625e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 36/300 Step: 131 loss : 0.2307 : 100%|██████████| 130/130 [01:40<00:00,  1.29it/s]\n",
      "epoch: 36/300 Step: 34 loss : 0.0823 : 100%|██████████| 33/33 [00:07<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/300], Validation Loss: 0.0823\n",
      "Current learning rate: 1.953125e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 37/300 Step: 131 loss : 0.2311 : 100%|██████████| 130/130 [01:40<00:00,  1.29it/s]\n",
      "epoch: 37/300 Step: 34 loss : 0.0849 : 100%|██████████| 33/33 [00:06<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/300], Validation Loss: 0.0849\n",
      "Current learning rate: 1.953125e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 38/300 Step: 131 loss : 0.2349 : 100%|██████████| 130/130 [01:39<00:00,  1.30it/s]\n",
      "epoch: 38/300 Step: 34 loss : 0.0822 : 100%|██████████| 33/33 [00:06<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/300], Validation Loss: 0.0822\n",
      "Current learning rate: 9.765625e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 39/300 Step: 131 loss : 0.2355 : 100%|██████████| 130/130 [01:39<00:00,  1.31it/s]\n",
      "epoch: 39/300 Step: 34 loss : 0.0839 : 100%|██████████| 33/33 [00:06<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/300], Validation Loss: 0.0839\n",
      "Current learning rate: 9.765625e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 40/300 Step: 131 loss : 0.2322 : 100%|██████████| 130/130 [01:38<00:00,  1.31it/s]\n",
      "epoch: 40/300 Step: 34 loss : 0.0833 : 100%|██████████| 33/33 [00:06<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/300], Validation Loss: 0.0833\n",
      "Current learning rate: 4.8828125e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 41/300 Step: 131 loss : 0.2330 : 100%|██████████| 130/130 [01:40<00:00,  1.30it/s]\n",
      "epoch: 41/300 Step: 34 loss : 0.0841 : 100%|██████████| 33/33 [00:06<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/300], Validation Loss: 0.0841\n",
      "Current learning rate: 4.8828125e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 42/300 Step: 131 loss : 0.2350 : 100%|██████████| 130/130 [01:40<00:00,  1.30it/s]\n",
      "epoch: 42/300 Step: 34 loss : 0.0855 : 100%|██████████| 33/33 [00:06<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/300], Validation Loss: 0.0855\n",
      "Current learning rate: 2.44140625e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 43/300 Step: 131 loss : 0.2385 : 100%|██████████| 130/130 [01:38<00:00,  1.32it/s]\n",
      "epoch: 43/300 Step: 34 loss : 0.0840 : 100%|██████████| 33/33 [00:06<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/300], Validation Loss: 0.0840\n",
      "Current learning rate: 2.44140625e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 44/300 Step: 131 loss : 0.2335 : 100%|██████████| 130/130 [01:39<00:00,  1.30it/s]\n",
      "epoch: 44/300 Step: 34 loss : 0.0844 : 100%|██████████| 33/33 [00:07<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/300], Validation Loss: 0.0844\n",
      "Current learning rate: 1.220703125e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 45/300 Step: 131 loss : 0.2345 : 100%|██████████| 130/130 [01:40<00:00,  1.29it/s]\n",
      "epoch: 45/300 Step: 34 loss : 0.0812 : 100%|██████████| 33/33 [00:07<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/300], Validation Loss: 0.0812\n",
      "Current learning rate: 1.220703125e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 46/300 Step: 131 loss : 0.2334 : 100%|██████████| 130/130 [01:39<00:00,  1.30it/s]\n",
      "epoch: 46/300 Step: 34 loss : 0.0843 : 100%|██████████| 33/33 [00:06<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/300], Validation Loss: 0.0843\n",
      "Current learning rate: 1.220703125e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 47/300 Step: 131 loss : 0.2338 : 100%|██████████| 130/130 [01:39<00:00,  1.30it/s]\n",
      "epoch: 47/300 Step: 34 loss : 0.0832 : 100%|██████████| 33/33 [00:06<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/300], Validation Loss: 0.0832\n",
      "Current learning rate: 1.220703125e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 48/300 Step: 131 loss : 0.2310 : 100%|██████████| 130/130 [01:40<00:00,  1.30it/s]\n",
      "epoch: 48/300 Step: 34 loss : 0.0837 : 100%|██████████| 33/33 [00:06<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/300], Validation Loss: 0.0837\n",
      "Current learning rate: 1.220703125e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 49/300 Step: 131 loss : 0.2340 : 100%|██████████| 130/130 [01:39<00:00,  1.31it/s]\n",
      "epoch: 49/300 Step: 34 loss : 0.0841 : 100%|██████████| 33/33 [00:07<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/300], Validation Loss: 0.0841\n",
      "Current learning rate: 1.220703125e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 50/300 Step: 131 loss : 0.2325 : 100%|██████████| 130/130 [01:38<00:00,  1.32it/s]\n",
      "epoch: 50/300 Step: 34 loss : 0.0841 : 100%|██████████| 33/33 [00:06<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/300], Validation Loss: 0.0841\n",
      "Current learning rate: 1.220703125e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 51/300 Step: 131 loss : 0.2341 : 100%|██████████| 130/130 [01:39<00:00,  1.31it/s]\n",
      "epoch: 51/300 Step: 34 loss : 0.0822 : 100%|██████████| 33/33 [00:06<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/300], Validation Loss: 0.0822\n",
      "Current learning rate: 1.220703125e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 52/300 Step: 131 loss : 0.2330 : 100%|██████████| 130/130 [01:39<00:00,  1.31it/s]\n",
      "epoch: 52/300 Step: 34 loss : 0.0847 : 100%|██████████| 33/33 [00:06<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/300], Validation Loss: 0.0847\n",
      "Current learning rate: 1.220703125e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 53/300 Step: 131 loss : 0.2360 : 100%|██████████| 130/130 [01:39<00:00,  1.31it/s]\n",
      "epoch: 53/300 Step: 34 loss : 0.0855 : 100%|██████████| 33/33 [00:06<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/300], Validation Loss: 0.0855\n",
      "Current learning rate: 1.220703125e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 54/300 Step: 131 loss : 0.2305 : 100%|██████████| 130/130 [01:39<00:00,  1.31it/s]\n",
      "epoch: 54/300 Step: 34 loss : 0.0830 : 100%|██████████| 33/33 [00:06<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/300], Validation Loss: 0.0830\n",
      "Current learning rate: 1.220703125e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 55/300 Step: 131 loss : 0.2319 : 100%|██████████| 130/130 [01:39<00:00,  1.31it/s]\n",
      "epoch: 55/300 Step: 34 loss : 0.0839 : 100%|██████████| 33/33 [00:06<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/300], Validation Loss: 0.0839\n",
      "Current learning rate: 1.220703125e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 56/300 Step: 131 loss : 0.2348 : 100%|██████████| 130/130 [01:39<00:00,  1.31it/s]\n",
      "epoch: 56/300 Step: 34 loss : 0.0854 : 100%|██████████| 33/33 [00:06<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/300], Validation Loss: 0.0854\n",
      "Current learning rate: 1.220703125e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 57/300 Step: 131 loss : 0.2336 : 100%|██████████| 130/130 [01:39<00:00,  1.31it/s]\n",
      "epoch: 57/300 Step: 34 loss : 0.0807 : 100%|██████████| 33/33 [00:06<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/300], Validation Loss: 0.0807\n",
      "Current learning rate: 1.220703125e-08\n",
      "Best model saved with Validation Loss: 2.663696263451129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 58/300 Step: 131 loss : 0.2383 : 100%|██████████| 130/130 [01:39<00:00,  1.31it/s]\n",
      "epoch: 58/300 Step: 34 loss : 0.0842 : 100%|██████████| 33/33 [00:06<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/300], Validation Loss: 0.0842\n",
      "Current learning rate: 1.220703125e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 59/300 Step: 131 loss : 0.2320 : 100%|██████████| 130/130 [01:39<00:00,  1.31it/s]\n",
      "epoch: 59/300 Step: 34 loss : 0.0844 : 100%|██████████| 33/33 [00:06<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59/300], Validation Loss: 0.0844\n",
      "Current learning rate: 1.220703125e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 60/300 Step: 131 loss : 0.2326 : 100%|██████████| 130/130 [01:39<00:00,  1.31it/s]\n",
      "epoch: 60/300 Step: 34 loss : 0.0834 : 100%|██████████| 33/33 [00:06<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/300], Validation Loss: 0.0834\n",
      "Current learning rate: 1.220703125e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 61/300 Step: 131 loss : 0.2332 : 100%|██████████| 130/130 [01:39<00:00,  1.31it/s]\n",
      "epoch: 61/300 Step: 34 loss : 0.0829 : 100%|██████████| 33/33 [00:06<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/300], Validation Loss: 0.0829\n",
      "Current learning rate: 1.220703125e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 62/300 Step: 43 loss : 0.2297 :  32%|███▏      | 42/130 [00:32<01:08,  1.28it/s]"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(params['epochs']):\n",
    "    # Training loop\n",
    "    train = tqdm(train_dataloader)\n",
    "    count = 0\n",
    "    train_loss = 0.0\n",
    "    decoder.train()  # Ensure the decoder is in training mode\n",
    "    encoder.train()\n",
    "    \n",
    "    for ct_images, cr_images, captions, lengths, sexs, ages, heights, weights in train:\n",
    "        ct_images, cr_images, captions = ct_images.to(device), cr_images.to(device), captions.to(device)\n",
    "        sexs, ages, heights, weights = sexs.to(device), ages.to(device), heights.to(device), weights.to(device)\n",
    "        count += 1\n",
    "\n",
    "        # Forward pass for encoder\n",
    "        features = encoder(cr_images, ct_images, torch.stack([sexs, ages, heights, weights], dim=1))\n",
    "        features = features.unsqueeze(1)  # [B, D] → [B, 1, D]\n",
    "        \n",
    "        # Prepare inputs for T5 decoder\n",
    "        labels = captions[:, 1:]          # Remove the first token for labels\n",
    "\n",
    "        # Forward pass through decoder with T5 format\n",
    "        outputs = decoder(encoder_hidden_states=features, decoder_input_ids=labels)\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), labels.contiguous().view(-1))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "        train.set_description(f\"epoch: {epoch+1}/{params['epochs']} Step: {count+1} loss : {train_loss/count:.4f} \")\n",
    "\n",
    "    # Validation loop\n",
    "    with torch.no_grad():\n",
    "        val_count = 0\n",
    "        val_loss = 0.0\n",
    "        val = tqdm(val_dataloader)\n",
    "        decoder.eval()  # Ensure the decoder is in evaluation mode\n",
    "        encoder.eval()\n",
    "        \n",
    "        for ct_images, cr_images, captions, lengths, sexs, ages, heights, weights in val:\n",
    "            val_count += 1\n",
    "            ct_images, cr_images, captions = ct_images.to(device), cr_images.to(device), captions.to(device)\n",
    "            sexs, ages, heights, weights = sexs.to(device), ages.to(device), heights.to(device), weights.to(device)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            features = encoder(cr_images, ct_images, torch.stack([sexs, ages, heights, weights], dim=1))\n",
    "            features = features.unsqueeze(1)  # [B, D] → [B, 1, D]\n",
    "            \n",
    "            # Validation inputs\n",
    "            labels = captions[:, 1:]\n",
    "\n",
    "            # Validation forward pass through decoder\n",
    "            outputs = decoder(encoder_hidden_states=features, decoder_input_ids=labels)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), labels.contiguous().view(-1))\n",
    "            val_loss += loss.item()\n",
    "            val.set_description(f\"epoch: {epoch+1}/{params['epochs']} Step: {val_count+1} loss : {val_loss/val_count:.4f} \")\n",
    "        \n",
    "        # Average validation loss\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{params['epochs']}], Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print(\"Current learning rate:\", optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        # Call scheduler with validation loss\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Save the model if it has the best validation loss so far\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(encoder.state_dict(), '../model/test_att_encoder.pth')\n",
    "            torch.save(decoder.state_dict(), '../model/test_att_decoder.pth')\n",
    "            print(\"Best model saved with Validation Loss:\", best_val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YS_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
